#!/usr/bin/env python3
"""
Malware scanner for unique AMI files
Runs YARA rules and ClamAV on extracted files from mounted AMI volumes

This scanner operates on static files on disk - the AMI is mounted but not booted,
so we're scanning filesystem artifacts, not running processes or memory.
"""
import os
import json
import subprocess
import argparse
import logging
import math
from pathlib import Path
from datetime import datetime

# Try to import yara, but don't fail if not available during testing
try:
    import yara
    YARA_AVAILABLE = True
except ImportError:
    YARA_AVAILABLE = False
    yara = None


def setup_logging(log_file=None):
    """Setup logging to console and optionally to file"""
    logger = logging.getLogger('malware_scanner')
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


def download_yara_rules(rules_bucket, region, local_path, logger):
    """Download YARA rules from S3 to local directory"""
    logger.info(f"Downloading YARA rules from s3://{rules_bucket}/yara-rules/ to {local_path}")
    try:
        result = subprocess.run([
            'aws', '--region', region, 's3', 'sync',
            f's3://{rules_bucket}/yara-rules/',
            local_path,
            '--exclude', '*.md',
            '--exclude', '*.txt',
            '--exclude', '.git/*'
        ], check=True, capture_output=True, text=True, timeout=300)
        logger.info(f"YARA rules downloaded successfully")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to download YARA rules: {e.stderr}")
        return False
    except subprocess.TimeoutExpired:
        logger.error("Timeout downloading YARA rules")
        return False


def compile_yara_rules(rules_dir, logger):
    """Compile all YARA rules in directory into a single rules object"""
    if not YARA_AVAILABLE:
        logger.warning("YARA module not available, skipping YARA compilation")
        return None

    rule_files = {}
    rules_path = Path(rules_dir)

    # Find all .yar and .yara files
    for pattern in ['**/*.yar', '**/*.yara']:
        for rule_file in rules_path.glob(pattern):
            # Use relative path as namespace to avoid collisions
            namespace = str(rule_file.relative_to(rules_path)).replace('/', '_').replace('.', '_')
            rule_files[namespace] = str(rule_file)

    if not rule_files:
        logger.warning(f"No YARA rule files found in {rules_dir}")
        return None

    logger.info(f"Compiling {len(rule_files)} YARA rule files...")

    # Compile rules, skipping any that fail
    compiled_rules = None
    successful_rules = {}
    failed_rules = []

    for namespace, filepath in rule_files.items():
        try:
            # Test compile each file individually first
            yara.compile(filepath=filepath)
            successful_rules[namespace] = filepath
        except yara.SyntaxError as e:
            failed_rules.append((namespace, str(e)))
        except Exception as e:
            failed_rules.append((namespace, str(e)))

    if failed_rules:
        logger.warning(f"Skipping {len(failed_rules)} rule files with errors")
        for name, err in failed_rules[:5]:  # Show first 5 errors
            logger.debug(f"  {name}: {err}")

    if successful_rules:
        try:
            compiled_rules = yara.compile(filepaths=successful_rules)
            logger.info(f"Successfully compiled {len(successful_rules)} YARA rule files")
        except Exception as e:
            logger.error(f"Failed to compile YARA rules: {e}")
            return None

    return compiled_rules


def scan_file_yara(rules, file_path, logger):
    """Scan a single file with YARA rules, return list of matches"""
    if rules is None:
        return []

    try:
        matches = rules.match(file_path, timeout=30)
        return [{
            'rule': m.rule,
            'tags': list(m.tags) if m.tags else [],
            'meta': dict(m.meta) if m.meta else {},
            'strings': [str(s) for s in (m.strings[:5] if m.strings else [])]  # First 5 matched strings
        } for m in matches]
    except yara.TimeoutError:
        logger.warning(f"YARA scan timeout for {file_path}")
        return []
    except Exception as e:
        logger.debug(f"YARA scan error for {file_path}: {e}")
        return []


def scan_file_clamav(file_path, logger):
    """Scan a single file with ClamAV, return detection name if found"""
    try:
        result = subprocess.run(
            ['clamscan', '--no-summary', '--infected', file_path],
            capture_output=True,
            text=True,
            timeout=60
        )
        # Return code 1 means virus found
        if result.returncode == 1:
            # Parse output to get detection name
            # Format: "/path/to/file: MalwareName FOUND"
            output = result.stdout.strip()
            if 'FOUND' in output:
                return output.split(':')[-1].replace('FOUND', '').strip()
        return None
    except subprocess.TimeoutExpired:
        logger.warning(f"ClamAV scan timeout for {file_path}")
        return None
    except FileNotFoundError:
        logger.warning("ClamAV (clamscan) not found, skipping AV scan")
        return None
    except Exception as e:
        logger.debug(f"ClamAV scan error for {file_path}: {e}")
        return None


def calculate_entropy(file_path, sample_size=1024*1024):
    """
    Calculate Shannon entropy of file (first sample_size bytes)

    High entropy (>7.5) indicates possibly packed/encrypted/compressed content
    which could be an attempt to hide malicious code.

    Entropy scale:
    - 0-4: Plain text, sparse data
    - 4-6: English text, typical data
    - 6-7.5: Compiled binaries, some compression
    - 7.5-8: High entropy - encrypted, packed, or compressed
    """
    try:
        with open(file_path, 'rb') as f:
            data = f.read(sample_size)

        if not data:
            return 0.0

        # Count byte frequencies
        freq = [0] * 256
        for byte in data:
            freq[byte] += 1

        # Calculate Shannon entropy
        entropy = 0.0
        data_len = len(data)
        for count in freq:
            if count > 0:
                p = count / data_len
                entropy -= p * math.log2(p)

        return entropy
    except Exception:
        return 0.0


def get_file_type(file_path):
    """Get file type using the 'file' command"""
    try:
        result = subprocess.run(
            ['file', '-b', file_path],
            capture_output=True,
            text=True,
            timeout=10
        )
        return result.stdout.strip()[:200]  # Truncate long descriptions
    except Exception:
        return "unknown"


def determine_severity(yara_matches, clamav_result, entropy, file_type):
    """
    Determine alert severity based on scan results

    Severity levels:
    - CRITICAL: ClamAV detection or high-confidence YARA malware matches
    - HIGH: YARA matches with malware-related tags
    - MEDIUM: Other YARA matches
    """
    # ClamAV detection is always critical
    if clamav_result:
        return 'CRITICAL'

    # Check YARA matches for severity indicators
    if yara_matches:
        high_severity_tags = {'malware', 'backdoor', 'webshell', 'miner', 'trojan',
                             'ransomware', 'apt', 'exploit', 'rootkit', 'rat'}
        critical_tags = {'CVE', 'cve'}

        for match in yara_matches:
            tags = set(tag.lower() for tag in match.get('tags', []))
            meta = match.get('meta', {})

            # Check for critical indicators in metadata
            if meta.get('severity', '').lower() == 'critical':
                return 'CRITICAL'

            # Check for critical tags
            if tags & critical_tags:
                return 'CRITICAL'

            # Check for high severity tags
            if tags & high_severity_tags:
                return 'HIGH'

        # Any YARA match is at least medium
        return 'MEDIUM'

    return 'LOW'


def scan_directory(extracted_dir, rules, output_file, logger):
    """
    Scan all files in directory with YARA and ClamAV

    Args:
        extracted_dir: Directory containing extracted unique files
        rules: Compiled YARA rules object
        output_file: Path to write JSON findings
        logger: Logger instance

    Returns:
        List of finding dictionaries
    """
    findings = []
    files_scanned = 0

    logger.info(f"Starting malware scan of {extracted_dir}")

    for root, dirs, files in os.walk(extracted_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            rel_path = os.path.relpath(file_path, extracted_dir)

            # Skip very large files (>100MB)
            try:
                file_size = os.path.getsize(file_path)
                if file_size > 100 * 1024 * 1024:
                    logger.debug(f"Skipping large file: {rel_path} ({file_size} bytes)")
                    continue
            except OSError:
                continue

            files_scanned += 1

            # Run YARA and ClamAV scans (entropy scanning disabled - too many false positives)
            yara_matches = scan_file_yara(rules, file_path, logger)
            clamav_result = scan_file_clamav(file_path, logger)
            file_type = get_file_type(file_path)

            # Only record findings if YARA or ClamAV detected something
            if yara_matches or clamav_result:
                severity = determine_severity(yara_matches, clamav_result, 0, file_type)

                finding = {
                    'file_path': rel_path,
                    'file_size': file_size,
                    'file_type': file_type,
                    'yara_matches': yara_matches,
                    'clamav_result': clamav_result,
                    'severity': severity,
                    'timestamp': datetime.utcnow().isoformat() + 'Z'
                }
                findings.append(finding)

                # Log findings
                logger.warning(f"[{severity}] {rel_path}: YARA={[m['rule'] for m in yara_matches]}, ClamAV={clamav_result}")

    logger.info(f"Scan complete: {files_scanned} files scanned, {len(findings)} findings")

    # Sort findings by severity
    severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
    findings.sort(key=lambda x: severity_order.get(x['severity'], 4))

    # Write findings to output file
    with open(output_file, 'w') as f:
        json.dump({
            'scan_timestamp': datetime.utcnow().isoformat() + 'Z',
            'files_scanned': files_scanned,
            'total_findings': len(findings),
            'findings_by_severity': {
                'CRITICAL': len([f for f in findings if f['severity'] == 'CRITICAL']),
                'HIGH': len([f for f in findings if f['severity'] == 'HIGH']),
                'MEDIUM': len([f for f in findings if f['severity'] == 'MEDIUM']),
                'LOW': len([f for f in findings if f['severity'] == 'LOW'])
            },
            'findings': findings
        }, f, indent=2)

    logger.info(f"Findings written to {output_file}")

    return findings


def main():
    parser = argparse.ArgumentParser(description='Scan extracted AMI files for malware')
    parser.add_argument('--extracted-dir', required=True, help='Directory containing extracted files')
    parser.add_argument('--rules-dir', default='/opt/yara-rules', help='Directory containing YARA rules')
    parser.add_argument('--output-file', required=True, help='Output JSON file for findings')
    parser.add_argument('--log-file', help='Optional log file path')
    parser.add_argument('--rules-bucket', help='S3 bucket to download YARA rules from')
    parser.add_argument('--region', default='us-west-2', help='AWS region for S3 operations')

    args = parser.parse_args()

    logger = setup_logging(args.log_file)

    # Download YARA rules from S3 if bucket specified
    if args.rules_bucket:
        if not download_yara_rules(args.rules_bucket, args.region, args.rules_dir, logger):
            logger.warning("Failed to download YARA rules, continuing with local rules if available")

    # Compile YARA rules
    rules = compile_yara_rules(args.rules_dir, logger)

    # Scan directory
    findings = scan_directory(args.extracted_dir, rules, args.output_file, logger)

    # Return exit code based on findings
    critical_count = len([f for f in findings if f['severity'] == 'CRITICAL'])
    high_count = len([f for f in findings if f['severity'] == 'HIGH'])

    if critical_count > 0:
        return 2  # Critical findings
    elif high_count > 0:
        return 1  # High severity findings
    return 0


if __name__ == "__main__":
    exit(main())
